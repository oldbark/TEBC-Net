PS Q:\知识工程\英文文献\KEQA\KEQA_WSDM19-master> sh main.sh
pred representation learning...
Word embedding match number 52282 out of 62662
Namespace(batch_size=32, best_prefix='pred', channel_size=8, clip_gradient=0.6, conv_kernel_1=3, conv_kernel_2=3, cuda=0, dev_every=10000, embed_dim=250, epochs=20, gpu=-1, hidden_size=300, label=250, log_every=2000, lr=0.0003, num_layer=2, output='preprocess', output_channel=300, patience=2, pool_kernel_1=3, pool_kernel_2=3, rel_maxlen=17, rnn_dropout=0.3, rnn_fc_dropout=0.3, seed=3435, seq_maxlen=21, train_embed=True, vector_cache='data/sq_glove300d.pt', weight_decay=0, words_dim=300, words_num=62662)
VOCAB num 62662
Train instance 229706
Dev instance 10845
cnn(
  (embed): Embedding(62662, 300)
  (gru): GRU(300, 300, num_layers=2, dropout=0.3, bidirectional=True)
  (mha): MultiHeadAttention(
    (WQ): Linear(in_features=600, out_features=600, bias=False)
    (WK): Linear(in_features=600, out_features=600, bias=False)
    (WV): Linear(in_features=600, out_features=600, bias=False)
    (scaled_dot_product_attn): ScaledDotProductAttention(
      (dropout): Dropout(p=0.1)
    )
    (linear): Linear(in_features=600, out_features=600, bias=False)
    (dropout): Dropout(p=0.1)
  )
  (cnnmha): MultiHeadAttention(
    (WQ): Linear(in_features=300, out_features=300, bias=False)
    (WK): Linear(in_features=300, out_features=300, bias=False)
    (WV): Linear(in_features=300, out_features=300, bias=False)
    (scaled_dot_product_attn): ScaledDotProductAttention(
      (dropout): Dropout(p=0.1)
    )
    (linear): Linear(in_features=300, out_features=300, bias=False)
    (dropout): Dropout(p=0.1)
  )
  (linear1): Linear(in_features=600, out_features=2048, bias=True)
  (batchnorm1d): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.3)
  (linear2): Linear(in_features=2048, out_features=250, bias=True)
  (linear3): Linear(in_features=900, out_features=2048, bias=True)
  (batchnormld2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear4): Linear(in_features=600, out_features=250, bias=True)
  (conv1): Conv2d(1, 300, kernel_size=(2, 300), stride=(1, 1), padding=(1, 0))
  (conv2): Conv2d(1, 300, kernel_size=(3, 300), stride=(1, 1), padding=(2, 0))
  (conv3): Conv2d(1, 300, kernel_size=(4, 300), stride=(1, 1), padding=(3, 0))
  (dropoutcnn): Dropout(p=0.5)
  (fc1): Linear(in_features=2048, out_features=250, bias=True)
  (cnnbatchnorm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
  Time Epoch Iteration Progress    (%Epoch)   Loss
     2     1         1     1/7179        0% 0.007958
  2242     1      2001  2001/7179       28% 0.001354
  4749     1      4001  4001/7179       56% 0.001409
  7408     1      6001  6001/7179       84% 0.000939
 10066     2      8001   822/7179       11% 0.000963
Dev loss: 0.0017038800534025804, accuracy: 0.7739972337482711
 12850     2     10001  2822/7179       39% 0.001148
 15563     2     12001  4822/7179       67% 0.000481
 18464     2     14001  6822/7179       95% 0.000808
 21303     3     16001  1643/7179       23% 0.000360
 24051     3     18001  3643/7179       51% 0.000447
Dev loss: 0.0014672973275239591, accuracy: 0.8021207929921623
 26856     3     20001  5643/7179       79% 0.000412
 29590     4     22001   464/7179        6% 0.001054
 32326     4     24001  2464/7179       34% 0.000306
 35057     4     26001  4464/7179       62% 0.000908
 37788     4     28001  6464/7179       90% 0.000345
Dev loss: 0.0013923256800142012, accuracy: 0.8101429230059936
 40583     5     30001  1285/7179       18% 0.000252
 43326     5     32001  3285/7179       46% 0.000577
 46072     5     34001  5285/7179       74% 0.000240
 48812     6     36001   106/7179        1% 0.000321
 51561     6     38001  2106/7179       29% 0.000309
Dev loss: 0.0014063779013501906, accuracy: 0.8061779621945597
 54380     6     40001  4106/7179       57% 0.000276
 57127     6     42001  6106/7179       85% 0.000745
 59832     7     44001   927/7179       13% 0.000355
 62521     7     46001  2927/7179       41% 0.000021
 65239     7     48001  4927/7179       69% 0.000515
Dev loss: 0.0014013860881785442, accuracy: 0.8094052558782849
 67978     7     50001  6927/7179       96% 0.000297
 70672     8     52001  1748/7179       24% 0.000942
 73425     8     54001  3748/7179       52% 0.000038
 76168     8     56001  5748/7179       80% 0.000630
 78843     9     58001   569/7179        8% 0.000272
Dev loss: 0.0015947034007128124, accuracy: 0.787551867219917
Early Stopping. Epoch: 9, Best Dev accuracy: 0.8101429230059936, loss: 0.0013923256800142012,
PS Q:\知识工程\英文文献\KEQA\KEQA_WSDM19-master> sh main.sh
We have to run train_detection.py, train_entity.py, train_pred.py first, before running test_main.py...
Q:\Python37\lib\site-packages\fuzzywuzzy\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
total num of example: 21687
Dev Precision:  91.032359% Recall:  94.615899% F1 Score:  92.789543%
line: 0
line: 1000000
line: 2000000
line: 3000000
line: 4000000
line: 5000000
316 out of 21687 nonmatching names, matching accuracy: 0.9854290588832019
recall of head entity prediction: 0.9365518513395121, num of mids per example 57.9723797666805
final accuracy: 0.746576289943284, head acc 0.8109005394937059, all acc 0.6729838151888228